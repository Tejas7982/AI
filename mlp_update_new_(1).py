# -*- coding: utf-8 -*-
"""MLP_update_new_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SFt--ddhWxlam4J45jzkZqENX9Lgsc8Q
"""

from google.colab import drive
drive.mount('/content/drive/')

import os
import librosa
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from joblib import Parallel, delayed

# Set the path to the directory containing the emotion folders
path_to_data = '/content/drive/MyDrive/Emotions'

# Define the features to extract
features = ['mfcc', 'hnr', 'zcr']

# Initialize empty lists for the features and labels
all_features = []
all_labels = []
all_filenames = []

# Define a function to extract the features from a single file
def extract_features(file_path):
    # Get the label for the current emotion folder
    label = os.path.basename(os.path.dirname(file_path))
    
    # Load the audio file using Librosa
    signal, sr = librosa.load(file_path, sr=22050)
    
    # Extract the desired features using Librosa
    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=30)

    # print(mfccs)
    hnr = librosa.effects.harmonic(signal)
    zcr = librosa.feature.zero_crossing_rate(signal)
    
    # Concatenate the features into a single vector
    current_features = []
    if 'mfcc' in features:
        current_features += list(np.mean(mfccs, axis=1))
    if 'hnr' in features:
        current_features += [np.mean(hnr)]
    if 'zcr' in features:
        current_features += list(np.mean(zcr, axis=1))
    
    # Get the filename
    filename = os.path.basename(file_path)
    
    # Return the features, label, and filename
    return (filename, current_features, label)


# Loop through each emotion folder and extract the features from each file
file_paths = [os.path.join(root, name) for root, dirs, files in os.walk(path_to_data) for name in files]
results = Parallel(n_jobs=-1, verbose=1)(delayed(extract_features)(file_path) for file_path in file_paths)
all_filenames, all_features, all_labels = zip(*results)

# Convert the feature and label lists to NumPy arrays
X = np.array(all_features)
y = np.array(all_labels)

# Shuffle the feature and label arrays
idx = np.arange(X.shape[0])
np.random.shuffle(idx)
X = X[idx]
y = y[idx]
filenames = np.array(all_filenames)
filenames = filenames[idx]

import pandas as pd

# create a list of column names for the 30 MFCCs
mfcc_columns = ['MFCC_' + str(i) for i in range(1, 31)]

# create a list of column names for the 3 other features
other_columns = ['HNR', 'ZCR']

# create a DataFrame from the X with the column names
df = pd.DataFrame(data=[[x for x in row] for row in X], columns=mfcc_columns + other_columns)
# convert numpy array y to dataframe
df1 = pd.DataFrame(y, columns =['Emotions'])
# create a DataFrame for filenames
df2 = pd.DataFrame(filenames, columns=['Filename'])
# concatenate all dataframes
data=pd.concat([df2, df, df1], axis=1)

# shuffle the data
data = data.sample(frac = 1)

# reset the index
data = data.reset_index(drop=True)
data=data.drop(columns=["Unnamed: 0"])

#saving the dataframe
data.to_csv('feature_extracted_file.csv')

import librosa
import soundfile
import os, glob, pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import os
import pandas as pd

### Load the dataset
data1 = pd.read_csv('/feature_extracted_file (1).csv')
data1=data1.drop(columns=["Unnamed: 0"])

data1.info()

data = data1.iloc[:, 1:35]

data

X = data.drop(columns=['Emotions'])
y = data['Emotions']

y

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#standardizing the dataset
mean = X_train.mean(axis=0)
X_train -= mean
std = X_train.std(axis=0)
X_train /= std
X_test -= mean
X_test /= std

X_train

y.value_counts()

#Initialize the Multi Layer Perceptron Classifier
model=MLPClassifier(alpha=0.01, batch_size=64, epsilon=1e-08, hidden_layer_sizes=(1500,), learning_rate='adaptive', max_iter=2500)

#Train the model
model.fit(X_train,y_train)

MLPClassifier(activation='relu', alpha=0.01, batch_size=64, beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(1500,), learning_rate='adaptive',
              learning_rate_init=0.001, max_fun=5000, max_iter=2500,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=None, shuffle=True, solver='adam',
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)

#Predict for the train set
y_pred=model.predict(X_train)

y_pred

#Calculate the accuracy of our model
accuracy=accuracy_score(y_true=y_train, y_pred=y_pred)

#Print the accuracy
print("Accuracy: {:.2f}%".format(accuracy*100))

from sklearn.metrics import accuracy_score, f1_score

f1_score(y_train, y_pred,average=None)

import pandas as pd
df=pd.DataFrame({'Actual': y_train, 'Predicted':y_pred})
df.head(20)

new_df1 = pd.concat([X_train, df],ignore_index=False,axis=1,sort=False)

new_df1.head(20)

new_df1.to_excel("/content/drive/MyDrive/X_train_Pred.xlsx")

#Test the model
model.fit(X_train,y_train)

MLPClassifier(activation='relu', alpha=0.01, batch_size=64, beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(1500,), learning_rate='adaptive',
              learning_rate_init=0.001, max_fun=5000, max_iter=2500,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=None, shuffle=True, solver='adam',
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)

#Predict for the test set
y_pred=model.predict(X_test)

y_pred

#Calculate the accuracy of our model
accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)

#Print the accuracy
print("Accuracy: {:.2f}%".format(accuracy*100))

f1_score(y_test, y_pred,average=None)

import pandas as pd
df2=pd.DataFrame({'Actual': y_test, 'Predicted':y_pred})
df2.head(20)

new_df2 = pd.concat([X_test, df2],ignore_index=False,axis=1,sort=False)

new_df2.head(20)

new_df2.to_excel("/content/drive/MyDrive/X_test_Pred.xlsx")

import pickle
# Writing different model files to file
with open( 'modelForPrediction1.sav', 'wb') as f:
    pickle.dump(model,f)

#import os

# Replace "file_name.pkl" with the name of your pickle file
#file_name = "modelForPrediction1.sav"

#for dirname, _, filenames in os.walk('/content'):
#    for filename in filenames:
 #       if filename == file_name:
  #          print(os.path.join(dirname, filename))

# Replace "file_name.pkl" with the name of your pickle file
file_name = "modelForPrediction1.sav"

from google.colab import files
files.download(file_name)

# Define the features to extract
#features = ['mfcc', 'hnr', 'zcr']

#def extract_features(audio_path):
    # Load the audio file using Librosa
 #   signal, sr = librosa.load(audio_path, sr=22050)
    
    # Extract the desired features using Librosa
  #  mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=30)
   # hnr = librosa.effects.harmonic(signal)
    #zcr = librosa.feature.zero_crossing_rate(signal)
    
    # Concatenate the features into a single vector
    #current_features = []
   # if 'mfcc' in features:
    #    current_features += list(np.mean(mfccs, axis=1))
    #if 'hnr' in features:
     #   current_features += [np.mean(hnr)]
    #if 'zcr' in features:
     #   current_features += list(np.mean(zcr, axis=1))
    
    # Return the features
    #return current_features

#(filename = 'modelForPrediction1.sav'
#loaded_model = pickle.load(open(filename, 'rb')) # loading the model file from the storage
#feature = extract_features(".mp3")

#feature = np.array(feature) 
#feature=feature.reshape(1,-1)

#prediction=loaded_model.predict(feature)
#prediction